{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4aa5362-0a47-45dd-946c-fb5e0871b738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face_recognition in c:\\users\\91997\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (8.1.7)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (19.24.99)\n",
      "Requirement already satisfied: numpy in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\91997\\appdata\\roaming\\python\\python312\\site-packages (from face_recognition) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91997\\anaconda3\\lib\\site-packages (from Click>=6.0->face_recognition) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install face_recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c79746-29db-4c41-ae54-8a2c56a170f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eab30f3-ca8c-4ada-a70c-9b6a86c8b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face_recognition in c:\\users\\91997\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (8.1.7)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (19.24.99)\n",
      "Requirement already satisfied: numpy in c:\\users\\91997\\anaconda3\\lib\\site-packages (from face_recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\91997\\appdata\\roaming\\python\\python312\\site-packages (from face_recognition) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91997\\anaconda3\\lib\\site-packages (from Click>=6.0->face_recognition) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea03181-c2e4-4739-91df-5f826acdaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5125911d-68f9-47c8-bb7d-3861074ff9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\91997\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\91997\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977b14e-8868-4852-98f7-f1271cef2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load a sample picture and learn how to recognize it \n",
    "known_image = face_recognition.load_image_file(r\"C:\\Users\\91997\\Desktop\\anshika.jpeg\")\n",
    "\n",
    "# Check if any faces are found in the image\n",
    "face_encodings = face_recognition.face_encodings(known_image)\n",
    "\n",
    "if len(face_encodings) > 0:\n",
    "    known_face_encoding = face_encodings[0]  # Take the first face encoding\n",
    "else:\n",
    "    print(\"Error: No faces found in the known image.\")\n",
    "    exit()\n",
    "\n",
    "# Extract the name from the image filename (without the extension)\n",
    "known_face_name = os.path.splitext(os.path.basename(r\"C:\\Users\\91997\\Desktop\\anshika.jpeg\"))[0]\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a single frame from the webcam\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    # Convert the frame from BGR to RGB (OpenCV uses BGR by default)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find all face locations and face encodings in the current frame \n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop through each face found in the current frame \n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces([known_face_encoding], face_encoding)\n",
    "        \n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        \n",
    "        # Use the known face name if there is a match, otherwise label as \"Unknown\"\n",
    "        name = known_face_name if matches[0] else \"Unknown\"\n",
    "        \n",
    "        # Label the face with the name or \"Unknown\"\n",
    "        cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' on the keyboard to exit the loop and shut down the camera\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc4479-ecce-4833-a0c0-db01daea1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path to the folder containing images\n",
    "images_folder = r\"C:\\Users\\91997\\Desktop\\face recognition\"\n",
    "\n",
    "# Initialize dictionaries to hold face encodings and names\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Load images from the folder and encode faces\n",
    "for filename in os.listdir(images_folder):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        image_path = os.path.join(images_folder, filename)\n",
    "        image = face_recognition.load_image_file(image_path)\n",
    "        face_encodings = face_recognition.face_encodings(image)\n",
    "\n",
    "        for face_encoding in face_encodings:\n",
    "            known_face_encodings.append(face_encoding)\n",
    "            known_face_names.append(os.path.splitext(filename)[0])\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a dictionary to track attendance\n",
    "attendance = {}\n",
    "\n",
    "# Define the Excel file path\n",
    "attendance_file = r\"C:\\Users\\91997\\Desktop\\face recognition\\anshi files.xlsx\"\n",
    "\n",
    "while True:\n",
    "    # Capture a single frame from the webcam\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Convert the frame from BGR to RGB (OpenCV uses BGR by default)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find all face locations and face encodings in the current frame\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop through each face found in the current frame\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face matches any known faces\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # Find the name of the matched face\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with the name or \"Unknown\"\n",
    "        cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Record attendance if the recognized name is known and not already recorded today\n",
    "        if name != \"Unknown\" and name not in attendance:\n",
    "            attendance[name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"Attendance recorded for {name} at {attendance[name]}\")\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' on the keyboard to exit the loop and shut down the camera\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the attendance data to an Excel file\n",
    "attendance_df = pd.DataFrame(list(attendance.items()), columns=['Name', 'Timestamp'])\n",
    "attendance_df.to_excel(attendance_file, index=False)\n",
    "print(f\"Attendance data saved to {attendance_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac6a28-7e83-4bd5-be25-7afadeed6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca2299-107a-4050-a0dd-8ec753a9d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c9157-b863-473a-8db0-1646baa5fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from fer import FER\n",
    "\n",
    "# Define the path to the folder containing images\n",
    "images_folder =  r\"C:\\Users\\91997\\Desktop\\face recognition\"\n",
    "\n",
    "# Initialize dictionaries to hold face encodings and names\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Load images from the folder and encode faces\n",
    "for filename in os.listdir(images_folder):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        image_path = os.path.join(images_folder, filename)\n",
    "        image = face_recognition.load_image_file(image_path)\n",
    "        face_encodings = face_recognition.face_encodings(image)\n",
    "\n",
    "        if not face_encodings:\n",
    "            print(f\"No faces found in image: {filename}\")\n",
    "        else:\n",
    "            for face_encoding in face_encodings:\n",
    "                known_face_encodings.append(face_encoding)\n",
    "                known_face_names.append(os.path.splitext(filename)[0])\n",
    "                print(f\"Loaded face encoding for: {filename}\")\n",
    "\n",
    "print(f\"Known faces: {known_face_names}\")\n",
    "\n",
    "if not known_face_encodings:\n",
    "    print(\"No known face encodings loaded.\")\n",
    "\n",
    "# Initialize the emotion detector\n",
    "emotion_detector = FER()\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a dictionary to track attendance\n",
    "attendance = {}\n",
    "\n",
    "# Define the Excel file path\n",
    "attendance_file =r\"C:\\Users\\91997\\Desktop\\face recognition\\anshi files.xlsx\"\n",
    "\n",
    "while True:\n",
    "    # Capture a single frame from the webcam\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame from webcam.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame from BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find all face locations and face encodings in the current frame\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    print(f\"Found {len(face_encodings)} faces in the current frame.\")\n",
    "\n",
    "    # Loop through each face found in the current frame\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face matches any known faces\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.6)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with the name or \"Unknown\"\n",
    "        cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Detect emotion\n",
    "        face = frame[top:bottom, left:right]\n",
    "        emotion, score = emotion_detector.top_emotion(face)\n",
    "        emotion_text = emotion if emotion else \"Unknown\"\n",
    "\n",
    "        # Display the emotion on the frame\n",
    "        cv2.putText(frame, f\"Emotion: {emotion_text}\", (left, top - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Record attendance if the recognized name is known and not already recorded today\n",
    "        if name != \"Unknown\" and name not in attendance:\n",
    "            attendance[name] = {\n",
    "                'Timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'Emotion': emotion_text\n",
    "            }\n",
    "            print(f\"Attendance recorded for {name} at {attendance[name]['Timestamp']} with emotion {emotion_text}\")\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' on the keyboard to exit the loop and shut down the camera\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the attendance data to an Excel file\n",
    "attendance_list = [{'Name': name, 'Timestamp': details['Timestamp'], 'Emotion': details['Emotion']} for name, details in attendance.items()]\n",
    "attendance_df = pd.DataFrame(attendance_list)\n",
    "attendance_df.to_excel(attendance_file, index=False)\n",
    "print(f\"Attendance data saved to {attendance_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9016fe-5c40-4f1d-8d09-972ad1a899f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5b801-c905-4332-afe5-b1de59451628",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install comtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c05fde-6ebf-480c-8d63-17de42b7a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pycaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69538a4-c4d5-4306-94b0-26a21098f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe opencv-python numpy pycaw comtypes screen-brightness-control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8908068-b3f3-4e93-bd0a-4303998592ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import screen_brightness_control as sbc\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "# Initialize Mediapipe and Pycaw\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Set up Pycaw for volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Get the volume range\n",
    "vol_range = volume.GetVolumeRange()\n",
    "min_vol = vol_range[0]\n",
    "max_vol = vol_range[1]\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "def volume_control_from_distance(distance, min_distance, max_distance):\n",
    "    # Normalize the distance\n",
    "    norm_distance = np.clip((distance - min_distance) / (max_distance - min_distance), 0, 1)\n",
    "    # Map the normalized distance to the volume range\n",
    "    return norm_distance * (max_vol - min_vol) + min_vol\n",
    "\n",
    "def brightness_control_from_distance(distance, min_distance, max_distance):\n",
    "    # Normalize the distance\n",
    "    norm_distance = np.clip((distance - min_distance) / (max_distance - min_distance), 0, 1)\n",
    "    # Map the normalized distance to the brightness range (0-100)\n",
    "    return int(norm_distance * 100)\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set up Mediapipe hand detection\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a later selfie-view display\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame and find hands\n",
    "        result = hands.process(rgb_frame)\n",
    "        \n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                # Draw the hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Get coordinates of the thumb tip (landmark 4) and index finger tip (landmark 8)\n",
    "                thumb_tip = hand_landmarks.landmark[4]\n",
    "                index_tip = hand_landmarks.landmark[8]\n",
    "                \n",
    "                # Convert normalized coordinates to pixel values\n",
    "                h, w, _ = frame.shape\n",
    "                thumb_tip_coords = (int(thumb_tip.x * w), int(thumb_tip.y * h))\n",
    "                index_tip_coords = (int(index_tip.x * w), int(index_tip.y * h))\n",
    "                \n",
    "                # Calculate the distance between thumb and index finger tips\n",
    "                distance = calculate_distance(thumb_tip_coords, index_tip_coords)\n",
    "                \n",
    "                # Map the distance to the volume range and set the system volume\n",
    "                vol = volume_control_from_distance(distance, 30, 200)  # Adjust min_distance and max_distance as needed\n",
    "                volume.SetMasterVolumeLevel(vol, None)\n",
    "                \n",
    "                # Map the distance to the brightness range and set the screen brightness\n",
    "                brightness = brightness_control_from_distance(distance, 30, 200)  # Adjust min_distance and max_distance as needed\n",
    "                sbc.set_brightness(brightness)\n",
    "                \n",
    "                # Display the distance, volume, and brightness on the frame\n",
    "                cv2.putText(frame, f'Distance: {int(distance)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'Volume: {int((vol - min_vol) / (max_vol - min_vol) * 100)}%', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'Brightness: {brightness}%', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        \n",
    "        # Display the frame with the contours drawn\n",
    "        cv2.imshow('Gesture Volume and Brightness Control', frame)\n",
    "        \n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
